{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "13ipus65lDtG",
        "outputId": "f91a3998-84c8-4a27-e7a3-11edbe29899f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2024-02-28 11:25:08--  http://ufldl.stanford.edu/housenumbers/train_32x32.mat\n",
            "Resolving ufldl.stanford.edu (ufldl.stanford.edu)... 171.64.68.10\n",
            "Connecting to ufldl.stanford.edu (ufldl.stanford.edu)|171.64.68.10|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 182040794 (174M) [text/plain]\n",
            "Saving to: ‘train_32x32.mat’\n",
            "\n",
            "train_32x32.mat      81%[===============>    ] 141.91M  4.70MB/s    eta 7s     "
          ]
        }
      ],
      "source": [
        "!wget http://ufldl.stanford.edu/housenumbers/train_32x32.mat\n",
        "!wget http://ufldl.stanford.edu/housenumbers/test_32x32.mat"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "CC5ZcxW8me40"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from scipy.io import loadmat\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from IPython import display\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "SFlfNpU-npHz"
      },
      "outputs": [],
      "source": [
        "trainset = loadmat(\"train_32x32.mat\")\n",
        "testset = loadmat(\"test_32x32.mat\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "wp8z2-oQn10o"
      },
      "outputs": [],
      "source": [
        "trainset['X'].shape, testset['X'].shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "y7Zp2pIfn--n"
      },
      "outputs": [],
      "source": [
        "idx = np.random.randint(0, trainset['X'].shape[3], size=36)\n",
        "fig, axes = plt.subplots(6, 6, sharex=True, sharey=True, figsize=(5, 5))\n",
        "for i, ax in zip(idx, axes.flatten()):\n",
        "  ax.imshow(trainset['X'][:, :, :, i], aspect='equal')\n",
        "  ax.xaxis.set_visible(False)\n",
        "  ax.yaxis.set_visible(False)\n",
        "plt.subplots_adjust(wspace=0, hspace=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "_51nmVETaUge"
      },
      "outputs": [],
      "source": [
        "def scale(x, feature_range=(-1, 1)):\n",
        "  x = (x - tf.reduce_min(x))/(255 - tf.reduce_min(x))\n",
        "\n",
        "  min, max = feature_range\n",
        "  x = x * (max - min) + min\n",
        "  return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "MPo6iRCJoUv_"
      },
      "outputs": [],
      "source": [
        "buffer_size = 70000\n",
        "BATCH_SIZE = 256\n",
        "train_dataset = tf.transpose(trainset['X'], (3, 0, 1, 2))\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices(train_dataset)\n",
        "train_dataset = train_dataset.map(map_func=scale, num_parallel_calls=tf.data.AUTOTUNE).shuffle(buffer_size).batch(BATCH_SIZE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "1O9onReVQlQZ"
      },
      "outputs": [],
      "source": [
        "imgs = next(iter(train_dataset))\n",
        "imgs.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "zhfoeuwleBXA"
      },
      "outputs": [],
      "source": [
        "imgs[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "MVfLBy9YRSa4"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "index = random.randint(0, imgs.shape[0]-1)\n",
        "plt.imshow(imgs[index])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "3CaBxclzSSlw"
      },
      "outputs": [],
      "source": [
        "def build_generator_seq():\n",
        "  model = tf.keras.Sequential()\n",
        "\n",
        "  model.add(tf.keras.layers.Dense(4 * 4 * 512, input_shape=(100, ), use_bias=False))\n",
        "  model.add(tf.keras.layers.BatchNormalization())\n",
        "  model.add(tf.keras.layers.LeakyReLU())\n",
        "\n",
        "  model.add(tf.keras.layers.Reshape((4, 4, 512)))\n",
        "\n",
        "  print(model.output_shape)\n",
        "\n",
        "  model.add(tf.keras.layers.Conv2DTranspose(256, (5, 5), strides=(2, 2), padding='same', use_bias=False))\n",
        "  model.add(tf.keras.layers.BatchNormalization())\n",
        "  model.add(tf.keras.layers.LeakyReLU())\n",
        "\n",
        "  print(model.output_shape)\n",
        "\n",
        "  model.add(tf.keras.layers.Conv2DTranspose(128, (5, 5), strides=(2, 2), padding='same', use_bias=False))\n",
        "  model.add(tf.keras.layers.BatchNormalization())\n",
        "  model.add(tf.keras.layers.LeakyReLU())\n",
        "\n",
        "  print(model.output_shape)\n",
        "\n",
        "  model.add(tf.keras.layers.Conv2DTranspose(3, (5, 5), strides=(2, 2), padding=\"same\"))\n",
        "  model.add(tf.keras.layers.Activation('tanh'))\n",
        "\n",
        "  print(model.output_shape)\n",
        "\n",
        "  return model\n",
        "\n",
        "def build_generator_func():\n",
        "  inputs = tf.keras.layers.Input(shape=(100,))\n",
        "  x = tf.keras.layers.Dense(4*4*512)(inputs)\n",
        "  x = tf.keras.layers.BatchNormalization()(x)\n",
        "  x = tf.keras.layers.LeakyReLU()(x)\n",
        "\n",
        "  x = tf.keras.layers.Reshape((4, 4, 512))(x)\n",
        "\n",
        "  x = tf.keras.layers.Conv2DTranspose(256, (5, 5), strides=(2, 2), padding='same', use_bias=True)(x)\n",
        "  x = tf.keras.layers.BatchNormalization()(x)\n",
        "  x = tf.keras.layers.LeakyReLU()(x)\n",
        "\n",
        "  x = tf.keras.layers.Conv2DTranspose(128, (5, 5), strides=(2, 2), padding='same')(x)\n",
        "  x = tf.keras.layers.BatchNormalization()(x)\n",
        "  x = tf.keras.layers.LeakyReLU()(x)\n",
        "\n",
        "\n",
        "\n",
        "  outputs = tf.keras.layers.Conv2DTranspose(3, (5,5), strides=(2, 2), padding='same')(x)\n",
        "  model = tf.keras.Model(inputs, outputs)\n",
        "  print(model.output_shape)\n",
        "  return model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "asFHRCf6WynX"
      },
      "outputs": [],
      "source": [
        "generator = build_generator_seq()\n",
        "generator.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2B0KuvTVZOWm"
      },
      "outputs": [],
      "source": [
        "noise = tf.random.normal((1, 100))\n",
        "img = generator(noise, training=False)\n",
        "plt.imshow(tf.squeeze(img), cmap='gray')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eTuSfMfDZfa1"
      },
      "outputs": [],
      "source": [
        "def build_discriminator():\n",
        "  model = tf.keras.Sequential()\n",
        "  model.add(tf.keras.layers.Conv2D(64, 5, strides=2, padding='same'))\n",
        "  model.add(tf.keras.layers.LeakyReLU())\n",
        "\n",
        "  model.add(tf.keras.layers.Conv2D(128, 5, strides=2, padding='same'))\n",
        "  model.add(tf.keras.layers.BatchNormalization())\n",
        "  model.add(tf.keras.layers.LeakyReLU())\n",
        "\n",
        "  model.add(tf.keras.layers.Conv2D(256, 5, strides=2, padding='same'))\n",
        "  model.add(tf.keras.layers.BatchNormalization())\n",
        "  model.add(tf.keras.layers.LeakyReLU())\n",
        "\n",
        "  model.add(tf.keras.layers.Flatten())\n",
        "  model.add(tf.keras.layers.Dense(1))\n",
        "  return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uYRa-L2xZ7Xt"
      },
      "outputs": [],
      "source": [
        "discriminator = build_discriminator()\n",
        "decision = discriminator(img)\n",
        "decision"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "brYdx3cIix1y"
      },
      "outputs": [],
      "source": [
        "cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
        "\n",
        "def discriminator_loss(real_output, fake_output):\n",
        "  real_loss = cross_entropy(tf.ones_like(real_output), real_output)\n",
        "  fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)\n",
        "\n",
        "  total_loss = real_loss + fake_loss\n",
        "  return total_loss\n",
        "\n",
        "\n",
        "def generator_loss(fake_output):\n",
        "  return cross_entropy(tf.ones_like(fake_output), fake_output)\n",
        "\n",
        "generator_optimizer = tf.keras.optimizers.Adam(1e-4)\n",
        "discriminator_optimizer = tf.keras.optimizers.Adam(1e-4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p5sG0qDNksvB"
      },
      "outputs": [],
      "source": [
        "EPOCHS = 50\n",
        "noise_dim = 100\n",
        "num_examples_to_generate = 16\n",
        "\n",
        "seed = tf.random.normal((num_examples_to_generate, noise_dim))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GyKoTvDilWwB"
      },
      "outputs": [],
      "source": [
        "@tf.function\n",
        "def train_step(images):\n",
        "  noise = tf.random.normal((BATCH_SIZE, noise_dim))\n",
        "  with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
        "    generated_imgs = generator(noise, training=True)\n",
        "\n",
        "    real_output = discriminator(images, training=True)\n",
        "    fake_output = discriminator(generated_imgs, training=True)\n",
        "\n",
        "    gen_loss = generator_loss(fake_output)\n",
        "    disc_loss = discriminator_loss(real_output, fake_output)\n",
        "\n",
        "  gradient_gen = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
        "  gradient_disc = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
        "\n",
        "  generator_optimizer.apply_gradients(zip(gradient_gen, generator.trainable_variables))\n",
        "  discriminator_optimizer.apply_gradients(zip(gradient_disc, discriminator.trainable_variables))\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XPmxjyB0pT4U"
      },
      "outputs": [],
      "source": [
        "def generate_and_save_images(model, epoch, test_input):\n",
        "  # Notice `training` is set to False.\n",
        "  # This is so all layers run in inference mode (batchnorm).\n",
        "  predictions = model(test_input, training=False)\n",
        "  fig = plt.figure(figsize=(4, 4))\n",
        "\n",
        "  for i in range(predictions.shape[0]):\n",
        "      plt.subplot(4, 4, i+1)\n",
        "      img = predictions[i]\n",
        "      img = tf.cast(((img - tf.reduce_min(img)) * 255 / (tf.reduce_max(img) - tf.reduce_min(img))), dtype=tf.uint8)\n",
        "      plt.imshow(img)\n",
        "      plt.axis('off')\n",
        "\n",
        "  plt.savefig('image_at_epoch_{:04d}.png'.format(epoch))\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2moyW_EKmzAC"
      },
      "outputs": [],
      "source": [
        "def train(dataset, epochs):\n",
        "  for epoch in range(epochs):\n",
        "    start = time.time()\n",
        "    for image_batch in dataset:\n",
        "      train_step(image_batch)\n",
        "      # display.clear_output(wait=True)\n",
        "\n",
        "      print(f'time for epoch {epoch + 1}, is {time.time() - start}')\n",
        "    # display.clear_output(wait=True)\n",
        "      generate_and_save_images(generator, epochs, seed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Vh-K-1Jkn7IC"
      },
      "outputs": [],
      "source": [
        "train(train_dataset, EPOCHS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "43oPoCmBoCS5"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}